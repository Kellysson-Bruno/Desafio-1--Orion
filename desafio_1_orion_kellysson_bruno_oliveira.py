# -*- coding: utf-8 -*-
"""Desafio 1 Orion - Kellysson Bruno Oliveira.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h-SzpXX4G8zUoUEgiy_Mgh6SYO8y--Im

##Desafio 1 - Orion

Aluno: Kellysson Bruno Oliveira

##Objetivo:
Construir e testar um classificador com Python usando o conjunto de dados Iris.



##Etapas:

1.Carregue o dataset Iris usando scikit-learn.

2.Explore os dados (gr치ficos, estat칤sticas descritivas).

3.Divida em treino e teste (ex: 70/30).

4.Treine um classificador simples:

Sugest칚o: KNeighborsClassifier ou DecisionTreeClassifier.

5.Avalie o modelo (accuracy, confusion matrix).

6.Salve o c칩digo no GitHub.

Extra (opcional):
Teste com outro classificador e compare.
Visualize a 치rvore de decis칚o ou os clusters.
"""

# Commented out IPython magic to ensure Python compatibility.
## NumPy para manipular matrizes e vetores
import numpy as np

## Pandas para manipular os dataframes
import pandas as pd

#importando Scikit learn dataset
from sklearn.datasets import load_iris

from sklearn import datasets

#dividindo dados em treino e teste
from sklearn.model_selection import train_test_split


# Importa칞칫es de modelos de classifica칞칚o B치sicos
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier

# Importa칞칫es de modelos de classifica칞칚o mais sofisticados
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import AdaBoostClassifier
!pip install xgboost
from xgboost import XGBClassifier
!pip install lightgbm
from lightgbm import LGBMClassifier
!pip install catboost
from catboost import CatBoostClassifier
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis


# Importa칞칫es dos par칙metros para avalia칞칚o dos modelos
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay


## Plotar gr치ficos
# %matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

import seaborn as sns

print('Setup completo!')

"""# ETAPA 1 - Entendendo o dataset IRIS"""

#Abrindo o iris dataset

iris = datasets.load_iris()
#df_iris = pd.DataFrame(iris.data, columns=iris.feature_names)
#df_iris.head()
df_iris = pd.DataFrame(iris.data, columns=iris.feature_names)
df_iris.head()

## Verificar informacoes gerais sobre as variaveis do dataset iris
#Objetivo:verificar se existe informa칞칚o faltante  e o tipo de dado presente no dataset

df_iris.info()

# Medidas de tendencia e dispers칚o
#Objetivo: compreender como est칚o distribu칤dos os dados em termos de normalidade

df_iris.describe()

# Distribuicao das frequencias
# Forma de observar a normalidade dos dados de cada uma das vari치veis presentes no dataset iris

df_iris.hist(bins=50, figsize=(20,15))
plt.show()

#Forma gr치fica 1 de identificar as vari치veis mais interessantes para a realiza칞칚o da tarefa de classifica칞칚o

# Carrega o dataset
iris = load_iris()

# Converte para DataFrame
df_iris = pd.DataFrame(iris.data, columns=iris.feature_names)
df_iris['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)

# Cria o pairplot
sns.pairplot(df_iris, hue='species', palette='Set1')

# Exibe o gr치fico
plt.suptitle('Gr치ficos de Dispers칚o - Todas as combina칞칫es das vari치veis (Iris)', y=1.02)
plt.show()

#Forma 2 de identificar as vari치veis mais interessantes para a realiza칞칚o da tarefa de classifica칞칚o

# Matriz de correla칞칚o
corr_matrix = df_iris.corr(numeric_only=True)
corr_matrix

#Forma gr치fica 2 de identificar as vari치veis mais interessantes para a realiza칞칚o da tarefa de classifica칞칚o - via identifica칞칚o visual da matriz de correla칞칚o

# Gera o gr치fico de correla칞칚o (heatmap)
plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Mapa de Correla칞칚o - Dataset Iris')
plt.show()

"""# Etapa 2 - Construindo os modelos de classifica칞칚o

Modelos de classifica칞칚o B치sicos
"""

#dividindo dados em treino e teste
from sklearn.model_selection import train_test_split

# Define X (vari치veis preditoras) e y (alvo/target)
X = iris.data              # Dados de entrada (4 vari치veis -  2 de p칠talas e duas de s칠talas)
y = iris.target            # R칩tulos (as 3 esp칠cies codificadas)


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""1. SVC Linear"""

# SVC Linear
print("\n游늷 Modelo: SVC Linear")
modelo_svc_linear = SVC(kernel='linear')
modelo_svc_linear.fit(X_train, y_train)
y_pred = modelo_svc_linear.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"游댳 Acur치cia: {acc:.2f}")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)
disp.plot(cmap='Blues')
plt.title("Matriz de Confus칚o - SVC Linear")
plt.show()

"""2. SVC RBF"""

# SVC RBF
print("\n游늷 Modelo: SVC RBF")
modelo_svc_rbf = SVC(kernel='rbf')
modelo_svc_rbf.fit(X_train, y_train)
y_pred = modelo_svc_rbf.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"游댳 Acur치cia: {acc:.2f}")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)
disp.plot(cmap='Blues')
plt.title("Matriz de Confus칚o - SVC RBF")
plt.show()

"""3. KNN"""

# KNN
print("\n游늷 Modelo: KNN")
modelo_knn = KNeighborsClassifier(n_neighbors=5)
modelo_knn.fit(X_train, y_train)
y_pred = modelo_knn.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"游댳 Acur치cia: {acc:.2f}")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)
disp.plot(cmap='Blues')
plt.title("Matriz de Confus칚o - KNN")
plt.show()

"""4. Decision Tree"""

# Decision Tree
print("\n游늷 Modelo: Decision Tree")
modelo_tree = DecisionTreeClassifier(random_state=42)
modelo_tree.fit(X_train, y_train)
y_pred = modelo_tree.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"游댳 Acur치cia: {acc:.2f}")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)
disp.plot(cmap='Blues')
plt.title("Matriz de Confus칚o - Decision Tree")
plt.show()

plt.figure(figsize=(12, 6))
plot_tree(modelo_tree, feature_names=iris.feature_names, class_names=target_names, filled=True)
plt.title("츼rvore de Decis칚o - Decision Tree")
plt.show()

"""5. Random Forest"""

# Random Forest
print("\n游늷 Modelo: Random Forest")
modelo_rf = RandomForestClassifier(n_estimators=100, random_state=42)
modelo_rf.fit(X_train, y_train)
y_pred = modelo_rf.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"游댳 Acur치cia: {acc:.2f}")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)
disp.plot(cmap='Blues')
plt.title("Matriz de Confus칚o - Random Forest")
plt.show()

plt.figure(figsize=(12, 6))
plot_tree(modelo_rf.estimators_[0], feature_names=iris.feature_names, class_names=target_names, filled=True)
plt.title("츼rvore de Decis칚o - 1춹 치rvore da Random Forest")
plt.show()

"""6. Logistic Regression"""

# Logistic Regression
print("\n游늷 Modelo: Logistic Regression")
modelo_logreg = LogisticRegression(max_iter=200)
modelo_logreg.fit(X_train, y_train)
y_pred = modelo_logreg.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"游댳 Acur치cia: {acc:.2f}")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)
disp.plot(cmap='Blues')
plt.title("Matriz de Confus칚o - Logistic Regression")
plt.show()

"""7. Naive Bayes"""

# Naive Bayes
print("\n游늷 Modelo: Naive Bayes")
modelo_nb = GaussianNB()
modelo_nb.fit(X_train, y_train)
y_pred = modelo_nb.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"游댳 Acur치cia: {acc:.2f}")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)
disp.plot(cmap='Blues')
plt.title("Matriz de Confus칚o - Naive Bayes")
plt.show()

"""8. MLP Classifier"""

# MLP Classifier
print("\n游늷 Modelo: MLP Classifier")
modelo_mlp = MLPClassifier(max_iter=1000)
modelo_mlp.fit(X_train, y_train)
y_pred = modelo_mlp.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"游댳 Acur치cia: {acc:.2f}")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)
disp.plot(cmap='Blues')
plt.title("Matriz de Confus칚o - MLP Classifier")
plt.show()

"""Modelos de classifica칞칚o mais sofisticados

1. Gradient Boosting
"""

#Gradient Boosting

model = GradientBoostingClassifier()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("Gradient Boosting - Acur치cia:", accuracy_score(y_test, y_pred))
ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=target_names)
plt.show()

"""2. AdaBoost"""

#AdaBoost

model = AdaBoostClassifier()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("AdaBoost - Acur치cia:", accuracy_score(y_test, y_pred))
ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=target_names)
plt.show()

"""3. XGBoost"""

#XGBoost

model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("XGBoost - Acur치cia:", accuracy_score(y_test, y_pred))
ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=target_names)
plt.show()

""" 4. LightGBM"""

# LightGBM

model = LGBMClassifier(verbose=-1)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("LightGBM - Acur치cia:", accuracy_score(y_test, y_pred))
ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=target_names)
plt.show()

"""5. CatBoost"""

# CatBoost

model = CatBoostClassifier(verbose=0)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("CatBoost - Acur치cia:", accuracy_score(y_test, y_pred))
ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=target_names)
plt.show()

"""6. Quadratic Discriminant Analysis (QDA)"""

# Quadratic Discriminant Analysis (QDA)

model = QuadraticDiscriminantAnalysis()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("QDA - Acur치cia:", accuracy_score(y_test, y_pred))
ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=target_names)
plt.show()

""" 7. Linear Discriminant Analysis (LDA)"""

# Linear Discriminant Analysis (LDA)

model = LinearDiscriminantAnalysis()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("LDA - Acur치cia:", accuracy_score(y_test, y_pred))
ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=target_names)
plt.show()

"""8. Bagging Classifier"""

# Bagging Classifier

model = BaggingClassifier( n_estimators=10, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("Bagging - Acur치cia:", accuracy_score(y_test, y_pred))
ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=target_names)
plt.show()

"""Conclus칚o:

Com exce칞칚o do modelo Naive Bayes, que teve uma acur치cia de 0.98; todos os outros modelos testados apresentaram uma acur치cia de 100%. Acredita-se que esses resultados podem ser explicados devido as caracteristicas limitadas do dataset.
"""